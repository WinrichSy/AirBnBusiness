{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run1\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from googletrans import Translator\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#models\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run2\n",
    "#Creates a text label above each bar in *rects*, displaying its height.\n",
    "def autolabel(rects, orientation='vert'):\n",
    "    #prints value above vertical bars\n",
    "    if orientation=='vert':\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(int(height)),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        weight = 'bold',\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom',\n",
    "                        size=15)\n",
    "    #Prints value to the right of horizontal bars\n",
    "    elif orientation=='hort':\n",
    "        for rect in rects:\n",
    "            width = rect.get_width()\n",
    "            ax.annotate('{}'.format(int(width)),\n",
    "                        xy=(width, rect.get_y() + rect.get_height() / 2),\n",
    "                        xytext=(3,-6),\n",
    "                        textcoords=\"offset points\",\n",
    "                        size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run3\n",
    "#Creates a text label above each bar in *rects*, displaying its height.\n",
    "def autolabel_percent(rects, orientation='vert'):\n",
    "    #Prints percentage above bars for vertical bars\n",
    "    if orientation=='vert':\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{0:.2f}%'.format(height),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        weight = 'bold',\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom',\n",
    "                        size=15)\n",
    "    #Prints percentages to the right of bars in horizontal bars\n",
    "    elif orientation=='hort':\n",
    "        for rect in rects:\n",
    "            width = rect.get_width()\n",
    "            ax.annotate(\"{0:.2f}%\".format(width),\n",
    "                        xy=(width, rect.get_y() + rect.get_height() / 2),\n",
    "                        xytext=(3,-6),\n",
    "                        textcoords=\"offset points\",\n",
    "                        size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run5\n",
    "#Print functions for bar graphs specifically\n",
    "def print_bar(x, y, title, x_label, y_label, title='insert title', orientation='vert', color='blue', \n",
    "              width=0.65, fig_size=(23,7), percentage=False, weight='bold', tick_size=20, title_size=30):\n",
    "    fig, ax = plt.subplots(figsize=fig_size)\n",
    "    \n",
    "    if orientation=='vert':\n",
    "        bars_for_annotation = ax.bar(x, y, color=color, align='center', width=width)\n",
    "    elif orientation=='hort':\n",
    "        bars_for_annotation = ax.barh(x, y, color=color, align='center')\n",
    "        \n",
    "    plt.xticks(size = 13, rotation=90)\n",
    "    plt.yticks(size = 15)\n",
    "    plt.xlabel(x_label, size=tick_size, color=color)\n",
    "    plt.ylabel(y_label, size=tick_size, color=color)\n",
    "    plt.title(title, fontsize=title_size, color=color)\n",
    "        \n",
    "    if percentage:\n",
    "        autolabel_percent(bars_for_annotation, orientation)\n",
    "    elif not percentage:\n",
    "        autolabel(bars_for_annotation, orientation)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: function for stemming\n",
    "def NLProcessing():\n",
    "    #do stuff\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO: preprocessing the data (uncomment later)\n",
    "# #Run6\n",
    "# useless_listing_columns = ['square_feet','country','listing_url','scrape_id','last_scraped','experiences_offered', 'license', \n",
    "#                    'xl_picture_url','host_url','host_name','host_thumbnail_url','street','host_listings_count',\n",
    "#                    'neighbourhood_group_cleansed','state','market', 'calendar_last_scraped','host_picture_url',\n",
    "#                    'host_acceptance_rate', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights',\n",
    "#                    'maximum_maximum_nights', 'jurisdiction_names', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes',\n",
    "#                    'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms','maximum_nights_avg_ntm',\n",
    "#                    'has_availability','country_code','host_acceptance_rate','thumbnail_url','medium_url','weekly_price','monthly_price',\n",
    "#                    'calendar_updated','is_business_travel_ready', 'interaction', 'interaction', 'house_rules']\n",
    "\n",
    "# def preprocessing(path):\n",
    "#     #load the LISTINGS csv\n",
    "#     processed_listings_df = pd.read_csv(path+'listings.csv')\n",
    "#     #drop useless columns\n",
    "#     processed_listings_df.drop(columns = useless_listing_columns, inplace=True)\n",
    "    \n",
    "#     #load the CALENDAR csv\n",
    "#     processed_calendar_df = pd.read_csv(path+'calendar.csv')\n",
    "#     #Convert date strings to datetime type\n",
    "#     date_time = pd.Series([datetime.strptime(x, '%Y-%m-%d') for x in processed_calendar_df['date']])\n",
    "#     processed_calendar_df['date'] = date_time\n",
    "    \n",
    "#     #load the REVIEWS csv\n",
    "#     processed_reviews_df = pd.read_csv(path+'reviews.csv')\n",
    "#     #Drop useless columns. Id of review and the reviewer's name\n",
    "#     processed_reviews_df.drop(columns=['id', 'reviewer_name'], inplace=True)\n",
    "\n",
    "#     return processed_listings_df, processed_calendar_df, processed_reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save new csv\n",
    "# singapore = 'Data/Singapore/'\n",
    "# london = 'Data/London/'\n",
    "# paris = 'Data/Paris/'\n",
    "\n",
    "# # paris_data_shortened.to_csv (paris+'shortened_listings.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary for ISO-639 to Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pypi.org/project/iso-639/\n",
    "from iso639 import languages\n",
    "\n",
    "#Create dictionary on types of languages available\n",
    "keys = ['af', 'am', 'an', 'ar', 'as', 'az', 'be', 'bg', 'bn', 'br', 'bs',\n",
    "        'ca', 'cs', 'cy', 'da', 'de', 'dz', 'el', 'en', 'eo', 'es', 'et', \n",
    "        'eu', 'fa', 'fi', 'fr', 'ga', 'gl', 'gu', 'he', 'hi', 'hr', 'ht',\n",
    "        'hu', 'hy', 'id', 'is', 'it', 'ja', 'jv', 'ka', 'kk', 'km', 'kn', \n",
    "        'ko', 'ku', 'ky', 'la', 'lb', 'lo', 'lt', 'lv', 'mg', 'mk', 'ml', \n",
    "        'mn', 'mr', 'ms', 'mt', 'nb', 'ne', 'nl', 'nn', 'no', 'oc', 'or', \n",
    "        'pa', 'pl', 'ps', 'pt', 'qu', 'ro', 'ru', 'rw', 'se', 'si', 'sk', \n",
    "        'sl', 'so', 'sq', 'sv', 'sw', 'ta', 'te', 'th', 'tl', 'tr', 'ug', \n",
    "        'uk', 'ur', 'vi', 'vo', 'wa', 'xh', 'zh', 'zu']\n",
    "\n",
    "#convert iso-639 to iso language name\n",
    "iso_name = [languages.get(alpha2=x).name for x in keys]\n",
    "\n",
    "language_dict = {}\n",
    "for idx, val in enumerate(keys):\n",
    "    language_dict[val] = iso_name[idx]\n",
    "    \n",
    "language_dict['nonsense'] = 'nonsense'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REVIEWS: CLEANING (PARIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pypi.org/project/langua/\n",
    "from langua import Predict\n",
    "p = Predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REVIEWS (PT1)\n",
    "#Load up Reviews (Paris)\n",
    "paris_reviews = pd.read_csv(paris + 'reviews.csv')\n",
    "\n",
    "#Drop useless columns. Id of review and the reviewer's name\n",
    "paris_reviews.drop(columns=['id', 'reviewer_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#REVIEWS (PT2)\n",
    "#detect language of reviewers\n",
    "#https://pypi.org/project/langdetect/\n",
    "import langdetect as ld\n",
    "#remove the rows with blank comments or not enough info\n",
    "paris_reviews = paris_reviews.loc[paris_reviews['comments'].notna()]\n",
    "# paris_reviews = paris_reviews.loc[paris_reviews['comments'].str.len()>20]\n",
    "\n",
    "review_language = []\n",
    "\n",
    "for idx, val in enumerate(paris_reviews['comments']):\n",
    "    try:\n",
    "        review_language.append(ld.detect(val))\n",
    "    except:\n",
    "        review_language.append('nonsense')\n",
    "\n",
    "# review_language = pd.Series([ld.detect(x[:50]) for x in paris_reviews['comments']])\n",
    "review_language = ['zh' if x=='zh-cn' or x=='zh-tw' else x for x in review_language]\n",
    "paris_reviews['language_code'] = review_language\n",
    "#DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_full_language = [language_dict[x] for x in review_language]\n",
    "paris_reviews['language'] = reviews_full_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REVIEWS (PT3)\n",
    "#Convert date strings to datetime type ###TODO check and update\n",
    "date_time = pd.Series([datetime.strptime(x, '%Y-%m-%d') for x in paris_reviews['date']])\n",
    "paris_reviews['date'] = date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all commas in comments\n",
    "reviews_no_commas = [x.replace(',','') for x in paris_reviews['comments']]\n",
    "paris_reviews['comments'] = reviews_no_commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove date in paris_reviews\n",
    "paris_reviews.drop(columns=['date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REVIEWS (PT4)\n",
    "paris_reviews.to_csv('Data/Paris/updated_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REVIEWS ARE CLEANED. NO NEED FOR MORE PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LISTINGS: CLEANING (PARIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/winrichsy/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#Load up listings\n",
    "paris_listings = pd.read_csv('Data/Paris/listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop useless columns (Paris)\n",
    "useless_columns = ['square_feet','country','listing_url','scrape_id','last_scraped','experiences_offered', 'license', \n",
    "                   'xl_picture_url','host_url','host_name','host_thumbnail_url','street','host_listings_count',\n",
    "                   'neighbourhood_group_cleansed','state','market', 'calendar_last_scraped','host_picture_url',\n",
    "                   'host_acceptance_rate', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights',\n",
    "                   'maximum_maximum_nights', 'jurisdiction_names', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes',\n",
    "                   'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms','maximum_nights_avg_ntm',\n",
    "                   'has_availability','country_code','host_acceptance_rate','thumbnail_url','medium_url','weekly_price','monthly_price',\n",
    "                   'calendar_updated','is_business_travel_ready', 'interaction', 'interaction', 'house_rules', 'experiences_offered']\n",
    "paris_listings_shortened = paris_listings.drop(columns = useless_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#detect language of hosts\n",
    "#1. Get only the description that are not empty\n",
    "#2. Get descriptions that are more than 20, overstepping random symbols\n",
    "paris_listings_shortened = paris_listings_shortened.loc[paris_listings_shortened['description'].notna()]\n",
    "paris_listings_shortened = paris_listings_shortened.loc[paris_listings_shortened['description'].str.len()>20]\n",
    "\n",
    "listing_language = [p.get_lang(x) for x in paris_listings_shortened['description']]\n",
    "listing_language = ['zh' if x=='zh-cn' or x=='zh-tw' else x for x in listing_language]\n",
    "\n",
    "paris_listings_shortened['language_code'] = listing_language\n",
    "#DO NOT RUN AGAIN UNTIL THE END: TAKES FOREVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_full_language = [language_dict[x] for x in listing_language]\n",
    "paris_listings_shortened['language'] = listing_full_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_columns = ['id', 'name', 'summary', 'space', 'description', 'host_id', 'host_since', 'host_about',\n",
    "                  'host_response_time', 'host_response_rate', 'host_is_superhost', 'host_total_listings_count',\n",
    "                  'host_verifications', 'host_has_profile_pic', 'host_identity_verified', 'latitude', 'longitude',\n",
    "                  'property_type', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'amenities', 'price',\n",
    "                  'security_deposit', 'cleaning_fee', 'minimum_nights', 'number_of_reviews', 'review_scores_rating',\n",
    "                  'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n",
    "                  'review_scores_location', 'review_scores_value', 'cancellation_policy', 'requires_license', 'reviews_per_month',\n",
    "                  'require_guest_phone_verification', 'language']\n",
    "updated_listings = paris_listings_shortened.loc[:,needed_columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading past updated csv file\n",
    "updated_listings = pd.read_csv('Data/Paris/updated_listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go through and replace all the nans\n",
    "no_nans_summary = ['nan' if isinstance(x,float) else x for x in updated_listings['summary']]\n",
    "updated_listings['summary'] = no_nans_summary\n",
    "\n",
    "no_nans_space = ['nan' if isinstance(x,float) else x for x in updated_listings['space']]\n",
    "updated_listings['space'] = no_nans_space\n",
    "\n",
    "no_nans_description = ['nan' if isinstance(x,float) else x for x in updated_listings['description']]\n",
    "updated_listings['description'] = no_nans_description\n",
    "\n",
    "no_nans_host_about = ['nan' if isinstance(x,float) else x for x in updated_listings['host_about']]\n",
    "updated_listings['host_about'] = no_nans_host_about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_commas(x):\n",
    "    return x.replace(',','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all commas for easier spark separator\n",
    "#name, summary, space, description, host_about\n",
    "#Todo: something about cancellation_policy\n",
    "check_string = ','\n",
    "\n",
    "summary_no_commas = [remove_commas(x) for x in updated_listings['summary']]\n",
    "updated_listings['summary'] = summary_no_commas\n",
    "\n",
    "space_no_commas = [remove_commas(x) for x in updated_listings['space']]\n",
    "updated_listings['space'] = space_no_commas\n",
    "\n",
    "description_no_commas = [remove_commas(x) for x in updated_listings['description']]\n",
    "updated_listings['description'] = description_no_commas\n",
    "\n",
    "host_about_no_commas = [remove_commas(x) for x in updated_listings['host_about']]\n",
    "updated_listings['host_about'] = host_about_no_commas\n",
    "\n",
    "notes_no_commas = [remove_commas(x) for x in updated_listings['notes']]\n",
    "updated_listings['notes'] = notes_no_commas\n",
    "\n",
    "transit_no_commas = [remove_commas(x) for x in updated_listings['transit']]\n",
    "updated_listings['transit'] = transit_no_commas\n",
    "\n",
    "access_no_commas = [remove_commas(x) for x in updated_listings['access']]\n",
    "updated_listings['access'] = access_no_commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save newly updated csv\n",
    "updated_listings.to_csv('Data/Paris/updated_listings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALENDAR: CLEANING (PARIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load up Calendar (Paris)\n",
    "paris_calendar = pd.read_csv(paris + 'calendar.csv')\n",
    "\n",
    "#Convert date strings to datetime type\n",
    "date_time = pd.Series([datetime.strptime(x, '%Y-%m-%d') for x in paris_calendar['date']])\n",
    "paris_calendar['date'] = date_time\n",
    "\n",
    "#DONT TOUCH\n",
    "paris_calendar.to_csv('Data/Paris/calendar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NPL pt2: Filter Tokens\n",
    "import string\n",
    "stopwords_ = set(stopwords.words('english'))\n",
    "punctuation_ = set(string.punctuation)\n",
    "\n",
    "def filter_tokens(sent):\n",
    "    return([w for w in sent if not w in stopwords_ and not w in punctuation_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NPL pt1: Remove Accents\n",
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_tokenize(sentence):\n",
    "    string = \" \"\n",
    "    \n",
    "    #remove accents\n",
    "    input_string = remove_accents(val)\n",
    "\n",
    "    #tokenize\n",
    "    sent_tokens = sent_tokenize(input_string)\n",
    "    tokens = [sent for sent in map(word_tokenize, sent_tokens)]\n",
    "    tokens_lower = [[word.lower() for word in sent] for sent in tokens]\n",
    "\n",
    "    #filtering stopwords and punctuations\n",
    "    tokens_filtered = list(map(filter_tokens, tokens_lower))\n",
    "    tokens_filtered_list = list(itertools.chain.from_iterable(tokens_filtered))\n",
    "\n",
    "    #stemming words\n",
    "    tokens_stemporter = [list(map(stemmer_snowball.stem, sent)) for sent in tokens_filtered]\n",
    "    return string.join(list(itertools.chain.from_iterable(tokens_stemporter)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP on only English [LISTINGS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_listings = updated_listings[updated_listings['language']=='English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_summary = []\n",
    "for idx, val in enumerate(english_listings['summary']):\n",
    "    tokenized_summary.append(personal_tokenize(val))\n",
    "\n",
    "english_listings['summary'] = tokenized_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_space = []\n",
    "for idx, val in enumerate(english_listings['space']):\n",
    "    tokenized_space.append(personal_tokenize(val))\n",
    "    \n",
    "english_listings['space'] = tokenized_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_description = []\n",
    "for idx, val in enumerate(english_listings['description']):\n",
    "    tokenized_description.append(personal_tokenize(val))\n",
    "    \n",
    "english_listings['description'] = tokenized_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_host_about = []\n",
    "for idx, val in enumerate(english_listings['host_about']):\n",
    "    tokenized_host_about.append(personal_tokenize(val))\n",
    "    \n",
    "english_listings['host_about'] = tokenized_host_about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Tokenizing Notes\n",
    "# tokenized_notes = []\n",
    "# for idx, val in enumerate(english_listings['notes']):\n",
    "#     tokenized_notes.append(personal_tokenize(val))\n",
    "    \n",
    "# english_listings['notes'] = tokenized_notes\n",
    "\n",
    "# #Tokenizing Transit\n",
    "# tokenized_transit = []\n",
    "# for idx, val in enumerate(english_listings['transit']):\n",
    "#     tokenized_transit.append(personal_tokenize(val))\n",
    "    \n",
    "# english_listings['transit'] = tokenized_transit\n",
    "\n",
    "# # Tokenizing Access\n",
    "# tokenized_access = []\n",
    "# for idx, val in enumerate(english_listings['access']):\n",
    "#     tokenized_access.append(personal_tokenize(val))\n",
    "    \n",
    "# english_listings['access'] = tokenized_access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save into new english only csv\n",
    "english_listings.to_csv('Data/Paris/english_listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#host verificaitons and ameneties. need to fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP on only English [REVIEWS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_reviews = pd.read_csv('Data/Paris/updated_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews = updated_reviews[updated_reviews['language']=='English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews.drop(columns = ['Unnamed: 0', 'language_code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize English reviews\n",
    "tokenized_comments = []\n",
    "for idx, val in enumerate(english_reviews['comments']):\n",
    "    tokenized_comments.append(personal_tokenize(val))\n",
    "\n",
    "english_reviews['comments'] = tokenized_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save into new english only csv\n",
    "english_reviews.to_csv('Data/Paris/english_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparkbook for SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run1\n",
    "#Run docker container\n",
    "sudo docker run -d --name sparkbook -p 8881:8888 -v \"$PWD\":/home/jovyan/work jupyter/pyspark-notebook start.sh jupyter lab --LabApp.token=''\n",
    "\n",
    "#Exec docker container\n",
    "docker exec -it sparkbook\n",
    "\n",
    "#Run Spark\n",
    "#go on browser\n",
    "localhost:8881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8913bf3bd976:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ParisAirBnb</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=ParisAirBnb>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run2\n",
    "import pyspark as ps\n",
    "\n",
    "spark = (ps.sql.SparkSession\n",
    "         .builder\n",
    "         .master('local[4]')\n",
    "         .appName('ParisAirBnb')\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run3\n",
    "paris = 'Data/Paris/'\n",
    "paris_listings = spark.read.csv(paris +'updated_listings.csv',\n",
    "                           header = True,\n",
    "                           sep = \"|\",\n",
    "                           inferSchema = True)\n",
    "\n",
    "paris_calendar = spark.read.csv(paris+'calendar.csv',\n",
    "                            header = True,\n",
    "                            sep = \"|\",\n",
    "                            inferSchema = True)\n",
    "\n",
    "paris_reviews = spark.read.csv(paris+'updated_reviews.csv',\n",
    "                                     header = True,\n",
    "                                     sep = \"|\",\n",
    "                                     inferSchema = True)\n",
    "\n",
    "#==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- space: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- host_since: string (nullable = true)\n",
      " |-- host_about: string (nullable = true)\n",
      " |-- host_response_time: string (nullable = true)\n",
      " |-- host_response_rate: string (nullable = true)\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- host_total_listings_count: string (nullable = true)\n",
      " |-- host_verifications: string (nullable = true)\n",
      " |-- host_has_profile_pic: string (nullable = true)\n",
      " |-- host_identity_verified: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- accommodates: string (nullable = true)\n",
      " |-- bathrooms: string (nullable = true)\n",
      " |-- bedrooms: string (nullable = true)\n",
      " |-- beds: string (nullable = true)\n",
      " |-- amenities: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- security_deposit: string (nullable = true)\n",
      " |-- cleaning_fee: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- number_of_reviews: string (nullable = true)\n",
      " |-- review_scores_rating: string (nullable = true)\n",
      " |-- review_scores_accuracy: string (nullable = true)\n",
      " |-- review_scores_cleanliness: string (nullable = true)\n",
      " |-- review_scores_checkin: string (nullable = true)\n",
      " |-- review_scores_communication: string (nullable = true)\n",
      " |-- review_scores_location: string (nullable = true)\n",
      " |-- review_scores_value: string (nullable = true)\n",
      " |-- cancellation_policy: string (nullable = true)\n",
      " |-- requires_license: string (nullable = true)\n",
      " |-- reviews_per_month: string (nullable = true)\n",
      " |-- require_guest_phone_verification: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paris_listings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- listing_id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- available: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- adjusted_price: string (nullable = true)\n",
      " |-- minimum_nights: double (nullable = true)\n",
      " |-- maximum_nights: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paris_calendar.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- reviewer_id: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- language_code: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paris_reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from googletrans import Translator\n",
    "# translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def translate_text(text, dest_language=\"en\"):\n",
    "#         # Used to translate using the googletrans library\n",
    "#         import json\n",
    "#         translator = Translator()\n",
    "#         try:\n",
    "#             translation = translator.translate(text=text, dest=dest_language)\n",
    "#         except json.decoder.JSONDecodeError:\n",
    "#             # api call restriction\n",
    "#             process = subprocess.Popen([\"/Applications/Surfshark.app\", \"d\"], stdout=subprocess.PIPE)\n",
    "#             process.wait()\n",
    "#             process = subprocess.Popen([\"/Applications/Surfshark.app\", \"c\", \"canada\"], stdout=subprocess.PIPE)\n",
    "#             process.wait()\n",
    "#             return Process_Data.translate_text(text=text, dest_language=dest_language)\n",
    "#         return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/Applications/Surfshark.app'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-a74cde7e2d01>\u001b[0m in \u001b[0;36mtranslate_text\u001b[0;34m(text, dest_language)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mformat_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlegacy_format_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mlegacy_format_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-584613b98c81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0menglish_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nonsense'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'language'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m'English'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0menglish_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0menglish_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-169-a74cde7e2d01>\u001b[0m in \u001b[0;36mtranslate_text\u001b[0;34m(text, dest_language)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;31m# api call restriction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"/Applications/Surfshark.app\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"d\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"/Applications/Surfshark.app\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"canada\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    773\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1520\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/Applications/Surfshark.app'"
     ]
    }
   ],
   "source": [
    "# import subprocess\n",
    "# summary = paris_listings_df[['summary', 'language']]\n",
    "# # english_text = [translator.translate(x['summary']).text if x['language']=='English' else x['summary'] for x in summary]\n",
    "# english_text = []\n",
    "# for i, j in summary.iterrows():\n",
    "# #     translator = Translator()\n",
    "# #     print('translating j', j)\n",
    "#     if isinstance(j['summary'], float):\n",
    "#         english_text.append('nonsense')\n",
    "#     elif j['language']!='English':\n",
    "#         english_text.append(translate_text(j['summary']))\n",
    "#     else:\n",
    "#         english_text.append(j['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_listings.createOrReplaceTempView('paris_listings')\n",
    "paris_calendar.createOrReplaceTempView('paris_calendar')\n",
    "paris_reviews.createOrReplaceTempView('paris_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Top 10 types of languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''SELECT DISTINCT(paris_listings.language), COUNT(language) as count\n",
    "            FROM paris_listings\n",
    "            GROUP BY language '''\n",
    "\n",
    "language_count_df = spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Percentage on Language Renters Speak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0 Get only english\n",
    "query = '''SELECT *\n",
    "            FROM paris_listings\n",
    "            WHERE language = \"English\"'''\n",
    "\n",
    "english_only = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.head of DataFrame[_c0: string, id: string, name: string, summary: string, space: string, description: string, host_id: string, host_since: string, host_about: string, host_response_time: string, host_response_rate: string, host_is_superhost: string, host_total_listings_count: string, host_verifications: string, host_has_profile_pic: string, host_identity_verified: string, latitude: string, longitude: string, property_type: string, accommodates: string, bathrooms: string, bedrooms: string, beds: string, amenities: string, price: string, security_deposit: string, cleaning_fee: string, minimum_nights: string, number_of_reviews: string, review_scores_rating: string, review_scores_accuracy: string, review_scores_cleanliness: string, review_scores_checkin: string, review_scores_communication: string, review_scores_location: string, review_scores_value: string, cancellation_policy: string, requires_license: string, reviews_per_month: string, require_guest_phone_verification: string, language: string]>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Data/Paris/updated_listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from class:\n",
    "def calculate_threshold_values(prob, y):\n",
    "    '''\n",
    "    Build dataframe of the various confusion-matrix ratios by threshold\n",
    "    from a list of predicted probabilities and actual y values\n",
    "    '''\n",
    "    df = pd.DataFrame({'prob': prob, 'y': y})\n",
    "    df.sort_values('prob', inplace=True)\n",
    "    \n",
    "    actual_p = df.y.sum()\n",
    "    actual_n = df.shape[0] - df.y.sum()\n",
    "\n",
    "    df['tn'] = (df.y == 0).cumsum()\n",
    "    df['fn'] = df.y.cumsum()\n",
    "    df['fp'] = actual_n - df.tn\n",
    "    df['tp'] = actual_p - df.fn\n",
    "\n",
    "    df['fpr'] = df.fp/(df.fp + df.tn)\n",
    "    df['tpr'] = df.tp/(df.tp + df.fn)\n",
    "    df['precision'] = df.tp/(df.tp + df.fp)\n",
    "    df = df.res\n",
    "    \n",
    "def plot_roc(ax, df):\n",
    "    ax.plot([1]+list(df.fpr), [1]+list(df.tpr), label=\"ROC\")\n",
    "    ax.plot([0,1],[0,1], 'k', label=\"random\")\n",
    "    ax.set_xlabel('fpr')\n",
    "    ax.set_ylabel('tpr')\n",
    "    ax.set_title('ROC Curve')\n",
    "    ax.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(text_series, y, vectorizer, model):\n",
    "    \n",
    "    X = vectorizer.fit_transform(text_series)\n",
    "    \n",
    "    if model.__class__.__name__ == \"GaussianNB\":\n",
    "        X = X.toarray()\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    probs = model.predict_proba(X_test)[:,1]\n",
    "    roc_auc = roc_auc_score(y_test, probs)\n",
    "    thresh_df =calculate_threshold_values(probs, y_test)\n",
    "    \n",
    "    return (precision, recall, accuracy, probs, roc_auc, thres_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTreeClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-288-8b7fe34ec179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mGNB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DecisionTreeClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "#models\n",
    "logistic = LogisticRegression()\n",
    "gb = GradientBoostingClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "GNB = GaussianNB()\n",
    "MNB - MultinomialNB()\n",
    "BNB = BernouliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('say \"Shit is done yo\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
