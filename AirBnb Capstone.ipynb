{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run docker container\n",
    "sudo docker run -d --name sparkbook -p 8881:8888 -v \"$PWD\":/home/jovyan/work jupyter/pyspark-notebook start.sh jupyter lab --LabApp.token=''\n",
    "\n",
    "#Exec docker container\n",
    "docker exec -it sparkbook\n",
    "\n",
    "#Run Spark\n",
    "#go on browser\n",
    "localhost:8881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run1\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run2\n",
    "import pyspark as ps\n",
    "\n",
    "spark = (ps.sql.SparkSession\n",
    "         .builder\n",
    "         .master('local[4]')\n",
    "         .appName('Instacart')\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/winrichsy/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "paris_listings = pd.read_csv('Data/Paris/listings.csv', nrows=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop useless columns\n",
    "useless_columns = ['square_feet','country','listing_url','scrape_id','last_scraped','experiences_offered', 'license', \n",
    "                   'xl_picture_url','host_url','host_name','host_thumbnail_url','street','host_listings_count',\n",
    "                   'neighbourhood_group_cleansed','state','market', 'calendar_last_scraped','host_picture_url',\n",
    "                   'host_acceptance_rate', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights',\n",
    "                   'maximum_maximum_nights', 'jurisdiction_names', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes',\n",
    "                   'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms','maximum_nights_avg_ntm',\n",
    "                   'has_availability','country_code','host_acceptance_rate','thumbnail_url','medium_url','weekly_price','monthly_price',\n",
    "                   'calendar_updated','is_business_travel_ready', 'interaction', 'interaction', 'house_rules']\n",
    "paris_data_shortened = paris_listings.drop(columns = useless_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save new csv\n",
    "singapore = 'Data/Singapore/'\n",
    "london = 'Data/London/'\n",
    "paris = 'Data/Paris/'\n",
    "\n",
    "paris_data_shortened.to_csv (paris+'shortened_listings.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run3\n",
    "paris_listings = spark.read.csv(paris +'shortened_listings.csv',\n",
    "                           header = True,\n",
    "                           sep = \",\",\n",
    "                           inferSchema = True)\n",
    "\n",
    "paris_calendar = spark.read.csv(paris+'shortened_calendar.csv',\n",
    "                            header = True,\n",
    "                            sep = \",\",\n",
    "                            inferSchema = True)\n",
    "\n",
    "paris_reviews = spark.read.csv(paris+'shortened_reviews.csv',\n",
    "                                     header = True,\n",
    "                                     sep = \",\",\n",
    "                                     inferSchema = True)\n",
    "\n",
    "#==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
